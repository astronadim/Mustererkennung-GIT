{"cells":[{"cell_type":"markdown","metadata":{"id":"ZuMMxZII8pI1"},"source":["##Imports"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31119,"status":"ok","timestamp":1685006895680,"user":{"displayName":"Dominik Laupheimer","userId":"04329458164899994368"},"user_tz":-120},"id":"eHE4r41P8aOk","outputId":"163c1914-0104-4b68-9b92-497d500c188b"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'google'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[7], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets, models, transforms\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Connect to google drive and import helper scripts\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m     23\u001b[0m a \u001b[38;5;241m=\u001b[39m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     24\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/MyDrive/ColabNotebooks/PatternRecognition_Lab3/src_python\u001b[39m\u001b[38;5;124m'\u001b[39m)\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"]}],"source":["\"\"\"\n","Stefan Schmohl, Dominik Laupheimer, \n","Institute for Photogrammetry, \n","University of Stuttgart, 2022\n","\"\"\"\n","\n","# # # # IMPORTS\n","import os\n","import time\n","import sklearn\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import ConfusionMatrixDisplay as CMD\n","from torch.utils.data import TensorDataset \n","from torch.utils.data import DataLoader \n","from torchvision import datasets, models, transforms\n","\n","# Connect to google drive and import helper scripts\n","from google.colab import drive\n","a = drive.mount('/content/drive')\n","os.chdir('/content/drive/MyDrive/ColabNotebooks/PatternRecognition_Lab3/src_python')\n","from utils import *"]},{"cell_type":"markdown","metadata":{"id":"DnZqF4P18yDk"},"source":["## Settings"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":227,"status":"ok","timestamp":1685007046134,"user":{"displayName":"Dominik Laupheimer","userId":"04329458164899994368"},"user_tz":-120},"id":"R6BVObMQdFHD"},"outputs":[],"source":["# # # # SETTINGS\n","class_names = {0: 'building', 1: 'car', 2: 'clutter'}   # label: name\n","\n","pretrained    = True    \n","architecture  = models.resnet18     # (function pointer)\n","lr = 0.001                          # learning rate\n","lr_step_size  = 20\n","lr_gamma      = 0.7\n","weight_decay  = 0.1\n","batch_size    = 32\n","device        = 'cuda'              # 'cuda' (GPU) or 'cpu'\n","\n","id_image_train = [2, 13]               # [row, column]\n","id_image_valid = [2, 15]               # [row, column] [2,15] randomly chosen\n","id_image_test  = [3, 14]               # [row, column]"]},{"cell_type":"markdown","metadata":{"id":"0Bf4NBYeIYDE"},"source":["## Loading the Training + Validation Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"executionInfo":{"elapsed":6483,"status":"ok","timestamp":1685007162794,"user":{"displayName":"Dominik Laupheimer","userId":"04329458164899994368"},"user_tz":-120},"id":"3wEoHzC6KIvf","outputId":"d730a3a5-3b92-4a98-8bde-a253bc085fd8"},"outputs":[{"ename":"NameError","evalue":"name 'load_crops' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m crops_train, gt_train \u001b[38;5;241m=\u001b[39m \u001b[43mload_crops\u001b[49m(\u001b[38;5;241m*\u001b[39mid_image_train)\n\u001b[0;32m      2\u001b[0m crops_valid, gt_valid \u001b[38;5;241m=\u001b[39m load_crops(\u001b[38;5;241m*\u001b[39mid_image_valid)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of input channels:      \u001b[39m\u001b[38;5;124m\"\u001b[39m, crops_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n","\u001b[1;31mNameError\u001b[0m: name 'load_crops' is not defined"]}],"source":["crops_train, gt_train = load_crops(*id_image_train)\n","crops_valid, gt_valid = load_crops(*id_image_valid)\n","\n","print(\"Number of input channels:      \", crops_train.shape[2])\n","print(\"Number of training samples:    \", crops_train.shape[-1])\n","print(\"Number of validation samples:  \", crops_valid.shape[-1])\n","\n","# Plot 8 random samples (RGB):\n","plot_samples(crops_train, gt_train, class_names, 8)"]},{"cell_type":"markdown","metadata":{"id":"0vNYrus24aDO"},"source":["## Preparing the Data for the Network (via `DataLoader`s)\n","Use `DataLoader` and `TensorDataset` to **feed data in a mini-batch-wise fashion** to the network. \n","\n","NOTE: Can also be used for data-augmentation, e.g., see `transforms` for `ImageFolder` dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":627,"status":"ok","timestamp":1685007400381,"user":{"displayName":"Dominik Laupheimer","userId":"04329458164899994368"},"user_tz":-120},"id":"Y7xsRnJuv_5_"},"outputs":[],"source":["def prep_data(images, labels, batch_size=64, shuffle=False):\n","  images_ = np.moveaxis(images, [0, 1, 2, 3], [2,3,1,0]) # (H,W,C,N) => (N,C​,H,W) \n","  images_ = torch.tensor(images_).float()\n","  labels_ = torch.tensor(labels).squeeze().long()\n","  dataset = TensorDataset(images_, labels_)\n","  dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n","  return dataloader\n","\n","# dataloader for training\n","dataloader_train = prep_data(crops_train, gt_train, batch_size=batch_size, shuffle=True)\n","\n","#\n","# TODO: create validation dataloader\n","#\n","# dataloader for validation\n","dataloader_val = prep_data(crops_val, gt_val, batch_size=batch_size, shuffle=False)\n"]},{"cell_type":"markdown","metadata":{"id":"2suLAyKbArGy"},"source":["## Defining Data Augmentations\n","Create a cascading list of augmentation transformations with out-of-the-box TorchVision random image transformation functions."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":221,"status":"ok","timestamp":1685007461002,"user":{"displayName":"Dominik Laupheimer","userId":"04329458164899994368"},"user_tz":-120},"id":"ZESpcRVuAs0X"},"outputs":[],"source":["aug_trafos = transforms.Compose([\n","  transforms.RandomHorizontalFlip(0.5),\n","  transforms.RandomVerticalFlip(0.5),\n","  transforms.RandomResizedCrop((crops_train.shape[0], crops_train.shape[1]), scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n","  transforms.RandomRotation(degrees=(-10, 10))\n","])"]},{"cell_type":"markdown","metadata":{"id":"DdKkGZldr0Rg"},"source":["## Creating the Network\n","**Download** the (pretrained) network.\n","\n","**Replace first layer** to accept `cin = 5 = [RGBIRnDSM]` input channels. **Replace last layer** to output probabilities for `cout = 3 = [building, car, clutter]` classes (and to remove the implicit 256x256 input size requirement).\n","\n","We make sure that all layers use the same format (`float()`) and move the network to the GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1650,"status":"ok","timestamp":1685007600273,"user":{"displayName":"Dominik Laupheimer","userId":"04329458164899994368"},"user_tz":-120},"id":"bXjueqSrr2ak","outputId":"50b20abb-3314-4ddf-e1b1-feb8e628ff0a"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n","100%|██████████| 44.7M/44.7M [00:00<00:00, 166MB/s]\n"]}],"source":["net = architecture(pretrained=pretrained, progress=True)\n","# print(net) # Display network architecture\n","cin = crops_train.shape[2]\n","cout = len(class_names)\n","\n","#\n","# TODO: replace first and last network layer.\n","#  - Find the names of those layers trough print(net)\n","#  - Redefine those layers with net.layername = torch.nn.XYZ(...)\n","#  - For the first layer (torch.nn.Conv2D), use the same parameters except the number of input channels (first parameter)\n","#  - For the last layer (torch.nn.Linear), use the same parameters except out_features.\n","#\n","\n","# Replace the first layer (name: 'conv1')\n","net.conv1 = torch.nn.Conv2d(cin, net.conv1.out_channels, kernel_size=net.conv1.kernel_size, stride=net.conv1.stride, padding=net.conv1.padding)\n","\n","# Replace the last layer (name: 'fc')\n","net.fc = torch.nn.Linear(net.fc.in_features, cout)\n","\n","# transform the model parameters to float\n","net.float();\n","\n","#\n","# TODO: transfer network to the device defined in settings.\n","#\n","net.to(device)\n"]},{"cell_type":"markdown","metadata":{"id":"2RY7VZs2ueKa"},"source":["## Training / Fine-Tuning\n","Use  \n","`net.train()` to put the network(-modules) into training mode and  \n","`net.eval()` respectively to activate the inference mode.  \n","The mode is relevant for specific layers, e.g., dropout and batch normalization."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":295},"executionInfo":{"elapsed":1320,"status":"error","timestamp":1685007731099,"user":{"displayName":"Dominik Laupheimer","userId":"04329458164899994368"},"user_tz":-120},"id":"uhkHmkf0IFeH","outputId":"302c50c6-456d-482d-e2cc-a37c6a3ebba3"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n","  warnings.warn(\n"]},{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-19b197265520>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# update tracking containers of the mini-batches (iterations)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0moa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mtrack_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtrack_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"]}],"source":["optimizer = torch.optim.SGD(net.parameters(), lr=lr, weight_decay=weight_decay)\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=lr_step_size, gamma=lr_gamma)\n","loss_fu = nn.CrossEntropyLoss()\n","\n","log = []\n","tic = time.time()\n","for i in range(100):\n","\n","  # # # # TRAINING (per epoch)\n","  net.train()\n","  track_loss = []  # running loss\n","  track_acc = []   # running accuracy\n","  \n","  # iterate over mini-batches\n","  for batch in dataloader_train:  # batch size has been defined while defining the DataLoader\n","    inputs = batch[0].to(device)\n","    labels = batch[1].to(device)\n","    inputs_augm = aug_trafos(inputs)    # apply data augmentations\n","\n","    #\n","    # TODO: training over each mini-batch (see demo script)\n","    # \n","    # Training over each mini-batch\n","    optimizer.zero_grad()  # Zero the gradients\n","    outputs = net(inputs_augm)  # Forward pass\n","    loss = loss_fu(outputs, labels)  # Compute loss\n","    loss.backward()  # Backward pass\n","    optimizer.step()  # Update weights\n","\n","\n","    # update tracking containers of the mini-batches (iterations)\n","    oa = torch.sum(torch.argmax(outputs,1) == labels).item() / len(labels) * 100\n","    track_loss.append(loss.item())\n","    track_acc.append(oa)\n","\n","\n","  # update tracked quantities (epochs)\n","  lr = optimizer.param_groups[0][\"lr\"]\n","  loss = np.mean(track_loss)\n","  acc = np.mean(track_acc)\n","  print(\"Epoch %2d,   lr: %.9f,   loss: %.4f,   acc: %5.1f\" % (i, lr, loss, acc), end='')\n","  log.append({'loss': loss, 'acc': acc})\n","  lr_scheduler.step()                   # decrease learning rate\n","\n","\n","  # # # # VALIDATION (per epoch)\n","  net.eval()\n","\n","\n","  #\n","  # TODO: run validation set trough the network. Record loss and accuracy\n","  #\n","  # Run validation set through the network\n","  valid_loss = []\n","  valid_acc = []\n","\n","  for batch in dataloader_val:\n","      inputs = batch[0].to(device)\n","      labels = batch[1].to(device)\n","      outputs = net(inputs)\n","      loss = loss_fu(outputs, labels)\n","      valid_loss.append(loss.item())\n","\n","      _, preds = torch.max(outputs, 1)\n","      acc = torch.sum(preds == labels).item() / len(labels) * 100\n","      valid_acc.append(acc)\n","\n","  valid_loss = np.mean(valid_loss)\n","  valid_acc = np.mean(valid_acc)\n","\n","\n","\n","  print(\",  valid_loss: %.4f,  valid_acc: %5.1f\" % (valid_loss, valid_acc))\n","  log[-1]['valid_loss'] = valid_loss\n","  log[-1]['valid_acc'] = valid_acc\n","\n","print(\"Finished Training after:\", time.time()-tic, \"sec.\")"]},{"cell_type":"markdown","metadata":{"id":"Ckco1RiaArxw"},"source":["## Plotting Training Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":404},"executionInfo":{"elapsed":1313,"status":"ok","timestamp":1668595268540,"user":{"displayName":"Dominik Laupheimer","userId":"04329458164899994368"},"user_tz":-60},"id":"8_M6TRCn0eTM","outputId":"8ecc0b87-d4c0-4be5-d782-ae746c9b5d17"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 1008x432 with 2 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n","def p(ax, d, t):\n","  ax.plot(d)\n","  ax.set_title(t)\n","  ax.set_xlabel('epoch')\n","  ax.grid(True)\n","p(ax1, [x['loss'] for x in log], 'loss')\n","p(ax2, [x['acc'] for x in log], 'acc [%]')\n","\n","#\n","# TODO: Plot valid metrics into same axes.\n","#\n","p(ax1, [x['valid_loss'] for x in log], 'valid loss')\n","p(ax2, [x['valid_acc'] for x in log], 'valid acc [%]')\n","\n","\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"PzNxUt2D41hC"},"source":["## Inferencing on Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":133},"executionInfo":{"elapsed":670,"status":"error","timestamp":1668595272616,"user":{"displayName":"Dominik Laupheimer","userId":"04329458164899994368"},"user_tz":-60},"id":"nmhVv1ZXNqEQ","outputId":"50f4f2a3-8405-4721-e294-af98990d4088"},"outputs":[{"ename":"SyntaxError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-9b5c900df256>\"\u001b[0;36m, line \u001b[0;32m23\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"]}],"source":["net.eval()\n","with torch.no_grad():\n","\n","  #\n","  # TODO: \n","  #  - Import test crops & create dataloader\n","  #  - Over all mini-batches:\n","  #     - Predict crop-wise network outputs for test dataset\n","  #     - collect the corresponding gt\n","  #  - Bring predictions & gt to cpu with .detach().cpu().\n","  #  - Use torch.nn.functional.softmax to convert the network outputs to probabilities.\n","  #  - Use torch.max to select per sample the class with the highest probability as prediction.\n","  #\n","  #  => variables (expected in further script):\n","  #        gt     = tensor of ground truth class labels per crop / bounding box\n","  #        preds  = tensor of predicted class label per crop / bounding box.\n","  #        probs  = tensor of probabilitiy of predicted class label per crop / bounding box.\n","  #\n","  # Tips:\n","  #   - use torch.cat() to concatinate list of tensors (per mini batch) to a single tensor.\n","  #   - max, argmax = torch.max()\n","  #\n","# Import test crops & create dataloader\n","  crops_test, gt_test = load_crops(id_image_test)  # Replace row and col with appropriate values\n","  dataloader_test = prep_data(crops_test, gt_test, batch_size=batch_size, shuffle=False)\n","\n","  gt = []\n","  preds = []\n","  probs = []\n","\n","  for batch in dataloader_test:\n","      inputs = batch[0].to(device)\n","      labels = batch[1].to(device)\n","      outputs = net(inputs)\n","      probabilities = torch.nn.functional.softmax(outputs, dim=1)\n","      _, predictions = torch.max(probabilities, 1)\n","\n","      gt.append(labels.detach().cpu())\n","      preds.append(predictions.detach().cpu())\n","      probs.append(probabilities.detach().cpu())\n","\n","  gt = torch.cat(gt)\n","  preds = torch.cat(preds)\n","  probs = torch.cat(probs)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IAzRl4Eb12tz"},"source":["## Evaluating"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":241},"executionInfo":{"elapsed":219,"status":"error","timestamp":1668595305660,"user":{"displayName":"Dominik Laupheimer","userId":"04329458164899994368"},"user_tz":-60},"id":"EmFOurSK140h","outputId":"7fa63b54-28fd-4719-8723-3ab130200597"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-f22722124051>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# performance metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0macc\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mrecall\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mf1\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'gt' is not defined"]}],"source":["# performance metrics\n","acc       = sklearn.metrics.accuracy_score(gt, preds)\n","recall    = sklearn.metrics.recall_score(gt, preds, average=None)\n","precision = sklearn.metrics.precision_score(gt, preds, average=None)\n","f1        = sklearn.metrics.f1_score(gt, preds, average=None)\n","print(\"Overall Test Accuracy: %5.1f %%\" % (acc*100))\n","print(\"Recall:    \", recall*100, '[%]')\n","print(\"Precision: \", precision*100, '[%]')\n","print(\"F1 score:  \", f1*100, '[%]')\n","\n","# confusion matrix\n","cm = confusion_matrix(gt, preds)\n","disp = CMD(confusion_matrix=cm, display_labels=list(class_names.values()))\n","disp = disp.plot(include_values=True, cmap='Greens', ax=None, xticks_rotation='horizontal')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"yNtKRk1AlZsH"},"source":["## Plotting the Test Results\n","After loading the **original bounding box coordinates** and the **RGB test image**, plot predictions onto RGB image and save to drive."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":223},"executionInfo":{"elapsed":4485,"status":"error","timestamp":1668595312283,"user":{"displayName":"Dominik Laupheimer","userId":"04329458164899994368"},"user_tz":-60},"id":"Ef-miRBnlCeL","outputId":"1cf72698-7de0-41de-9221-2039554a82fc"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-04618d397412>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%d_%d '\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplot_bbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m,\u001b[0m           \u001b[0mcolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' GT'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplot_bbs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' Prediction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'gt' is not defined"]}],"source":["img = load_img(*id_image_test)\n","bbs = load_bbs(*id_image_test)\n","\n","img_name = '%d_%d ' % tuple(id_image_test)\n","colors = ['b', 'y', 'r']\n","plot_bbs(img, bbs, gt,           colors=colors, title=img_name + ' GT')\n","plot_bbs(img, bbs, preds, probs, colors=colors, title=img_name + ' Prediction')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
